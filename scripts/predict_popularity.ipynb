{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "URI = \"\"\n",
    "DB_NAME = \"\"\n",
    "COLLECTION_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 18)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the desired collection.\n",
    "client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "db = client[DB_NAME]\n",
    "writer_info = db[COLLECTION_NAME]\n",
    "\n",
    "# Prepare the raw dataset.\n",
    "dataset = {\"followers_count\": [],\n",
    "            \"publication_following_count\": [],\n",
    "            \"has_twitter_username\": [],  # convert from string to bool\n",
    "            \"is_writer_program_enrolled\": [],\n",
    "            \"allow_notes\": [],\n",
    "            \"medium_member_at\": [],  # need to calculate the time later\n",
    "            \"is_book_author\": [],\n",
    "            \"title\": [],  # need to do NLP later\n",
    "            \"subtitle\": [],  # need to do NLP later\n",
    "            #\"tags\": [],\n",
    "            #\"topics\": [],\n",
    "            \"word_count\": [],\n",
    "            \"reading_time\": [],\n",
    "            \"is_series\": [],\n",
    "            \"is_shortform\": [],\n",
    "            \"top_highlight\": [],  # need to do NLP later\n",
    "            \"content\": [],  # need to do NLP later\n",
    "            \"claps\": [],\n",
    "            \"voters\": [],\n",
    "            \"responses_count\": []}\n",
    "\n",
    "for writer in writer_info.find():\n",
    "    for article in writer[\"top_articles\"]:\n",
    "        dataset[\"followers_count\"].append(writer.get(\"followers_count\", 0))\n",
    "        dataset[\"publication_following_count\"].append(writer.get(\"publication_following_count\", 0))\n",
    "        dataset[\"has_twitter_username\"].append(writer.get(\"twitter_username\", \"\").strip() != \"\")\n",
    "        dataset[\"is_writer_program_enrolled\"].append(writer.get(\"is_writer_program_enrolled\", False))\n",
    "        dataset[\"allow_notes\"].append(writer.get(\"allow_notes\", False))\n",
    "        dataset[\"medium_member_at\"].append(writer.get(\"medium_member_at\", \"\").strip())\n",
    "        dataset[\"is_book_author\"].append(writer.get(\"is_book_author\", False))\n",
    "        dataset[\"title\"].append(article.get(\"title\", \"\").strip())\n",
    "        dataset[\"subtitle\"].append(article.get(\"subtitle\", \"\").strip())\n",
    "        dataset[\"word_count\"].append(article.get(\"word_count\", 0))\n",
    "        dataset[\"reading_time\"].append(article.get(\"reading_time\", 0))\n",
    "        dataset[\"is_series\"].append(article.get(\"is_series\", False))\n",
    "        dataset[\"is_shortform\"].append(article.get(\"is_shortform\", False))\n",
    "        dataset[\"top_highlight\"].append(article.get(\"top_highlight\", \"\").strip())\n",
    "        dataset[\"claps\"].append(article.get(\"claps\", 0))\n",
    "        dataset[\"voters\"].append(article.get(\"voters\", 0))\n",
    "        dataset[\"responses_count\"].append(article.get(\"responses_count\", 0))\n",
    "\n",
    "        if \"content\" in article:\n",
    "            dataset[\"content\"].append(article[\"content\"].get(\"content\", \"\").strip())\n",
    "        else:\n",
    "            dataset[\"content\"].append(\"\")\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 19)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert medium_member_at to medium_usage_time in days.\n",
    "\n",
    "medium_usage_time = []\n",
    "current_date = datetime.strptime(\"2024-02-24 16:45:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "for medium_member_at in dataset[\"medium_member_at\"]:\n",
    "    if medium_member_at == \"\":\n",
    "        medium_usage_time.append(0)\n",
    "        continue\n",
    "\n",
    "    date = datetime.strptime(medium_member_at, \"%Y-%m-%d %H:%M:%S\")\n",
    "    medium_usage_time.append((current_date - date).days)\n",
    "dataset[\"medium_usage_time\"] = medium_usage_time\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do tfidf to all text fields.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def sanitize(texts):\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        text = t.lower()\n",
    "        text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "        text = re.sub('\\\\s+', ' ', text)\n",
    "        text = nlp(text)\n",
    "        tokens = [words.lemma_ for words in text if not words.is_stop and len(words) >= 2]\n",
    "        result.append(\" \".join(tokens))\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "def vectorize(col_name, max_num):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "    contents = sanitize(dataset[col_name].values)\n",
    "    tfidf_matrix = vectorizer.fit_transform(contents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_sum = np.sum(tfidf_matrix, axis=0)\n",
    "    tfidf_sum_array = np.squeeze(np.asarray(tfidf_sum))\n",
    "    tfidf_mapping = dict(zip(feature_names, tfidf_sum_array))\n",
    "    sorted_tfidf = sorted(tfidf_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = sorted_tfidf[:max_num]\n",
    "    indices = [np.where(feature_names == word)[0][0] if word in feature_names else None for word, _ in top_words]\n",
    "    return tfidf_matrix.toarray()[:, indices]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def vectorize(col_name, max_num):\n",
    "    vectorizer = CountVectorizer(stop_words='english', token_pattern=r'\\b[a-zA-Z]+\\b', max_features=max_num)\n",
    "    contents = sanitize(dataset[col_name].values)  # Assuming sanitize is a function defined elsewhere to clean the data\n",
    "    bow_matrix = vectorizer.fit_transform(contents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    bow_sum = np.sum(bow_matrix, axis=0)\n",
    "    bow_sum_array = np.squeeze(np.asarray(bow_sum))\n",
    "    bow_mapping = dict(zip(feature_names, bow_sum_array))\n",
    "    sorted_bow = sorted(bow_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word for word, _ in sorted_bow[:max_num]]\n",
    "    indices = [np.where(feature_names == word)[0][0] for word in top_words if word in feature_names]\n",
    "\n",
    "    return bow_matrix.toarray()[:, indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 219)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def update_dataset_with_vector(dataset, col_name, max_num):\n",
    "    dataset_copy = dataset.copy(deep=True)\n",
    "    vector = vectorize(col_name, max_num)\n",
    "    n_cols = vector.shape[1]\n",
    "    for i in range(n_cols):\n",
    "        dataset_copy[f\"{col_name}_{i}\"] = vector[:, i]\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "dataset_final = update_dataset_with_vector(dataset, \"title\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"subtitle\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"top_highlight\", 50)\n",
    "dataset_final = update_dataset_with_vector(dataset_final, \"content\", 50)\n",
    "print(dataset_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: -5.1539585876700915\n",
      "Random Forest R2: -0.49848399093787465\n",
      "Linear Regression R2: -86.12345607788389\n",
      "Random Forest R2: -0.9960375779385318\n",
      "Linear Regression R2: -30.4181894953775\n",
      "Random Forest R2: -0.639998402518845\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DataType(Enum):\n",
    "    TEXT_ONLY = 0\n",
    "    TEXT_EXCLUSIVE = 1\n",
    "    ALL_DATA = 2\n",
    "\n",
    "\n",
    "def get_X(X, data_type=DataType.ALL_DATA):\n",
    "    non_text_cols = [\"followers_count\",\n",
    "                    \"publication_following_count\",\n",
    "                    \"has_twitter_username\",\n",
    "                    \"is_writer_program_enrolled\",\n",
    "                    \"allow_notes\",\n",
    "                    \"medium_usage_time\",\n",
    "                    \"is_book_author\",\n",
    "                    \"word_count\",\n",
    "                    \"reading_time\",\n",
    "                    \"is_series\",\n",
    "                    \"is_shortform\"]\n",
    "    X_new = X.copy(deep=True)\n",
    "    if data_type == DataType.TEXT_ONLY:\n",
    "        X_new = X_new.drop(columns=non_text_cols)\n",
    "    elif data_type == DataType.TEXT_EXCLUSIVE:\n",
    "        X_new = X_new[non_text_cols]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# Try linear regression\n",
    "def fit_linear_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    #y_pred = model.predict(X_test)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"Linear Regression R2: {score}\")\n",
    "\n",
    "\n",
    "def fit_random_forest(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = RandomForestRegressor(n_estimators=50, max_depth=10,ccp_alpha = 0.1, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    print(f\"Random Forest R2: {score}\")\n",
    "\n",
    "\n",
    "col_to_exclude = [\"medium_member_at\", \"title\", \"subtitle\", \"top_highlight\", \"content\", \"claps\", \"voters\", \"responses_count\"]\n",
    "X = dataset_final.drop(columns=col_to_exclude)\n",
    "y_claps = dataset_final[\"claps\"]\n",
    "y_voters = dataset_final[\"voters\"]\n",
    "y_responses_count = dataset_final[\"responses_count\"]\n",
    "fit_linear_regression(X, y_claps)\n",
    "fit_random_forest(X, y_claps)\n",
    "fit_linear_regression(X, y_voters)\n",
    "fit_random_forest(X, y_voters)\n",
    "fit_linear_regression(X, y_responses_count)\n",
    "fit_random_forest(X, y_responses_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: -0.00765671804985546\n",
      "Linear Regression R2: -8.639614592524273\n",
      "Linear Regression R2: -157.34656788575552\n",
      "Random Forest R2: -0.24787309809866764\n",
      "Random Forest R2: -0.5212665105266716\n",
      "Random Forest R2: -0.49848399093787465\n",
      "Linear Regression R2: 0.09314989992237821\n",
      "Linear Regression R2: -5.859667442244723\n",
      "Linear Regression R2: -13.221653008252407\n",
      "Random Forest R2: -0.33432336053336065\n",
      "Random Forest R2: -0.9292821806806537\n",
      "Random Forest R2: -0.9960375779385318\n",
      "Linear Regression R2: -0.18592815951993558\n",
      "Linear Regression R2: -2.7694406378671044\n",
      "Linear Regression R2: -29.794874733304002\n",
      "Random Forest R2: -3.508273576845453\n",
      "Random Forest R2: -1.5350751230060418\n",
      "Random Forest R2: -0.639998402518845\n"
     ]
    }
   ],
   "source": [
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE), y_claps)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_claps)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_claps)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_claps)\n",
    "\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE),y_voters)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_voters)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_voters)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_voters)\n",
    "\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE),y_responses_count)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y_responses_count)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y_responses_count)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y_responses_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_responses_count + y_claps + y_voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: -0.48592464832560145\n",
      "Linear Regression R2: -40.61016108560024\n",
      "Linear Regression R2: -4.987688407814307\n",
      "Random Forest R2: -0.28255120391952593\n",
      "Random Forest R2: -0.6100022776167733\n",
      "Random Forest R2: -0.59277713999234\n"
     ]
    }
   ],
   "source": [
    "fit_linear_regression(get_X(X, DataType.TEXT_EXCLUSIVE),y)\n",
    "fit_linear_regression(get_X(X, DataType.TEXT_ONLY), y)\n",
    "fit_linear_regression(get_X(X, DataType.ALL_DATA), y)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_EXCLUSIVE), y)\n",
    "fit_random_forest(get_X(X, DataType.TEXT_ONLY), y)\n",
    "fit_random_forest(get_X(X, DataType.ALL_DATA), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
